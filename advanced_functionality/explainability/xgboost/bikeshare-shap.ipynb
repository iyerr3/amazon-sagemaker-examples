{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.\n",
    "\n",
    "In this notebook, we'll be building a simple regression model to predict hourly bike rentals using a popular dataset. There are two goals for this notebook:\n",
    "\n",
    "1. Show how XGBoost can be used to build a regression model\n",
    "2. Describe various SHAP visualizations we can build to understand our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This dataset contains daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal is to predict how many bikes will be rented depending on the weather and the day. The data can be downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset).\n",
    "\n",
    "Features available in the dataset: \n",
    "\n",
    "- Count of bicycles including both casual and registered users. The count is used as the target in the regression task.\n",
    "- The season, either spring, summer, fall or winter.\n",
    "- Indicator whether the day was a holiday or not.\n",
    "- The year, either 2011 or 2012.\n",
    "- The hour of day : 0 to 23\n",
    "- Number of days since the 01.01.2011 (the first day in the dataset).\n",
    "- Indicator whether the day was a working day or weekend.\n",
    "- The weather situation on that day. One of:\n",
    "    - clear, few clouds, partly cloudy, cloudy\n",
    "    - mist + clouds, mist + broken clouds, mist + few clouds, mist\n",
    "    - light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds\n",
    "    - heavy rain + ice pallets + thunderstorm + mist, snow + mist\n",
    "- Temperature in degrees Celsius.\n",
    "- Relative humidity in percent (0 to 100).\n",
    "- Wind speed in km per hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and imports\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import pickle as pkl                              # For serializing/deserializing model\n",
    "import sagemaker                                  # Amazon SageMaker's Python SDK provides many helper functions\n",
    "import os                                         # For manipulating filepath names\n",
    "from sklearn.model_selection import train_test_split   # For splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role: automatically get the role if run on a SageMaker instance\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# role = get_execution_role()\n",
    "role = \"arn:aws:iam::398557468931:role/service-role/AmazonSageMaker-ExecutionRole-20190626T094141\"\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define S3 details for storing the dataset ######\n",
    "\n",
    "BUCKET = 'xgboost-examples-1'\n",
    "PREFIX = 'sagemaker/DEMO-bike-shap'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the dataset available from UCI's ML Repository and upload to S3 after some transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
    "!unzip -o Bike-Sharing-Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hour.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "1. Replace column names with meaningful names\n",
    "2. One-hot encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'weathersit':'weather',\n",
    "                     'mnth':'month',\n",
    "                     'hr':'hour',\n",
    "                     'hum': 'humidity',\n",
    "                     'cnt':'count'},\n",
    "            inplace=True)\n",
    "# drop columns that don't add value or are directly correlated to output \n",
    "data = data.drop(['registered', 'casual', 'instant','dteday','yr'], axis=1)\n",
    "\n",
    "# encode all categorical columns\n",
    "categorical_columns = ['season', 'month', 'hour', 'holiday', 'weekday', 'weather']\n",
    "categorical_col_values = {'season': ['spring', 'summer', 'fall', 'winter'], \n",
    "                          'month': ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'],\n",
    "                          'hour': range(24), \n",
    "                          'holiday': ['no', 'yes'], \n",
    "                          'weekday': ['sun', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat'], \n",
    "                          'weather': ['clear', 'mist', 'snow', 'rain']}\n",
    "\n",
    "data_dummy = pd.get_dummies(data, prefix=categorical_columns, columns=categorical_columns, drop_first=True)\n",
    "data_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_dummy.loc[:, \"count\"]\n",
    "X = data_dummy.drop(\"count\", axis=1)\n",
    "# create a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "# Use 'csv' format to store the data\n",
    "# The first column is expected to be the output column\n",
    "pd.concat([y_train, X_train], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([y_test, X_test], axis=1).to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input files to XGBoost cannot contain headers. Hence, we use a separate file to store the feature names ensuring our analysis contains meaningful names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('feature_names.csv', 'w', newline='') as f: \n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow(X.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(BUCKET).Object(os.path.join(prefix, 'data/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(BUCKET).Object(os.path.join(prefix, 'data/test.csv')).upload_file('test.csv')\n",
    "boto3.Session().resource('s3').Bucket(BUCKET).Object(os.path.join(prefix, 'data/feature_names.csv')).upload_file('feature_names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Source distributed script mode\n",
    "from sagemaker.session import s3_input, Session\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "# use validation set to choose # of trees\n",
    "hyperparams = {\n",
    "    \"eta\": 0.2,\n",
    "    \"max_depth\": 4, \n",
    "    \"subsample\": 0.5, \n",
    "    \"verbose\": 0, \n",
    "    'num_round': 500\n",
    "}\n",
    "\n",
    "instance_type = \"ml.c5.2xlarge\"\n",
    "output_path = 's3://{}/{}'.format(BUCKET, PREFIX)\n",
    "content_type = \"csv\"\n",
    "train_input = s3_input(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'data/train.csv'), content_type=content_type)\n",
    "test_input = s3_input(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'data/test.csv'), content_type=content_type)\n",
    "feature_names_input = s3_input(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'data/feature_names.csv'), content_type=content_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "session = Session(boto_session=boto_session)\n",
    "script_path = 'bikeshare-script.py'\n",
    "\n",
    "xgb_script_mode_estimator = XGBoost(\n",
    "    entry_point=script_path,\n",
    "    framework_version='0.90-1', # Note: framework_version is mandatory\n",
    "    hyperparameters=hyperparams,\n",
    "    role=role,\n",
    "    train_instance_count=1, \n",
    "    train_instance_type=instance_type,\n",
    "    output_path=output_path)\n",
    "\n",
    "xgb_script_mode_estimator.fit({'train': train_input, \n",
    "                               'feature_names': feature_names_input,\n",
    "                               'test': test_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below script displays the bucket and path that contains the model artifact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "MODEL_FILE_PATH = urlparse(xgb_script_mode_estimator.model_data, allow_fragments=False).path.lstrip('/')\n",
    "print(\"Model artifact details \\n BUCKET = '{}' \\n MODEL_FILE_PATH = '{}'\".format(BUCKET, MODEL_FILE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability for XGBoost models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xgboost graphviz shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import boto3\n",
    "import pickle as pkl    \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3client = boto3.client('s3')\n",
    "s3client.download_file(BUCKET, MODEL_FILE_PATH, 'model.tar.gz')\n",
    "!tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following parameters are used in the notebook visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model (XGBoost booster) file ## \n",
    "model = pkl.load(open('xgboost_model', 'rb'))\n",
    "feature_names = model.feature_names\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shap values ##\n",
    "shap_values = pkl.load(open('shap_values', 'rb'))\n",
    "N_ROWS = shap_values.shape[0]\n",
    "N_FEATURES = shap_values.shape[1]\n",
    "\n",
    "baseline_value = shap_values[0, -1]  # last column is the baseline\n",
    "shap_values = np.delete(shap_values, -1, axis=1) # remove the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference dataset used for SHAP computation ## \n",
    "\n",
    "# This dataset is assumed to *NOT* contain the output label \n",
    "# and have the same number of features as the shap_values\n",
    "data = pkl.load(open('reference_data', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree plot\n",
    "\n",
    "An XGBoost model consists of an ensemble of classification and regression trees (CART).  Change the `NUM_TREE` value (ordinal number of the target tree) to plot other trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "NUM_TREE = 4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "xgb.plot_tree(model, num_trees=NUM_TREE, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Feature importances\n",
    "\n",
    "XGBoost provides multiple feature importance metrics to understand the influence of each feature on the model.\n",
    "\n",
    "- *gain*: contribution of feature to the model calculated by taking the increase in prediction accuracy ('total_gain' is the sum of all gain across all splits, 'gain' is the average of all gain across all splits)\n",
    "\n",
    "- *cover*: coverage of a feature calculated by the number of data points affected by a split involving the feature ('total_cover' is the sum of all coverage, 'cover' is the average across all splits)\n",
    "\n",
    "- *weight*: percentage representing the relative number of times a particular feature occurs in the trees of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10\n",
    "\n",
    "fig = plt.figure(figsize=(18, 15))\n",
    "fig.suptitle('Feature importances for the top {} features'.format(MAX_FEATURES), fontsize=24)\n",
    "\n",
    "for index, type in enumerate(('gain', 'total_gain', 'cover', 'total_cover', 'weight'), start=1): \n",
    "    ax = fig.add_subplot(3, 2, index)\n",
    "    xgb.plot_importance(model, importance_type=type, title=type,\n",
    "                        ax=ax, grid=False, height=0.4, \n",
    "                        max_num_features=MAX_FEATURES, show_values=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the model using SHAP\n",
    "\n",
    "We use SHAP values as means to understand the contributions of the features to the model predictions. SHAP (SHapley Additive exPlanations) by Lundberg and Lee (2016) is a method to\n",
    "explain individual predictions and is based on the game theoretically optimal\n",
    "Shapley Values.\n",
    "\n",
    "A prediction can be explained by assuming that each feature value of the\n",
    "instance is a “player” in a game where the prediction is the payout. Shapley\n",
    "values – a method from coalitional game theory – tells us how to fairly\n",
    "distribute the “payout” among the features.\n",
    "\n",
    "Reference: \"Explainable machine-learning predictions for the prevention of hypoxaemia during surgery\", Nature Biomedical Engineering, 2018\n",
    "\n",
    "------\n",
    "\n",
    "**Be careful to interpret the Shapley value correctly**: The Shapley value is the\n",
    "average contribution of a feature value to the prediction in different\n",
    "coalitions. The Shapley value is NOT the difference in prediction when we would\n",
    "remove the feature from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP summary\n",
    "\n",
    "A global aggregation of the individual Shapley values gives the overall average contributions of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, feature_names=feature_names, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **summary plot** below can provide more context over the bar chart of feature importances. It tells which features are most important, and also their range of effects over the dataset. The color allows us to match how changes in the value of a feature effect the change in risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features=data, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP dependence plots\n",
    "\n",
    "A **SHAP dependence plot** shows how the model output varies by feature value while showing the interaction between two features. The feature used for coloring is automatically chosen to highlight what might be driving these interactions. This shows how the model depends on the given feature, and can be considered a richer extenstion of the classical parital dependence plots. Vertical dispersion of the data points represents interaction effects. Grey ticks along the y-axis are data points where the feature’s value was NaN.\n",
    "\n",
    "Here we plot the top ranked features  (ordered by mean absolute SHAP value over all the samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RANKS = 3\n",
    "for r in range(N_RANKS):\n",
    "    shap.dependence_plot(\"rank({})\".format(r), \n",
    "                         shap_values, \n",
    "                         data, \n",
    "                         feature_names=feature_names,\n",
    "                         interaction_index='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP force plots\n",
    "\n",
    "A **force plot** explanation shows how features are contributing to push the model output from the base value (the average model output over the dataset) to the actual prediction. Features pushing the prediction higher are shown in **red**, those pushing the prediction lower are in **blue**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N_ROWS = shap_values.shape[1]\n",
    "N_SAMPLES = min(1000, N_ROWS)\n",
    "sampled_indices = np.random.randint(N_ROWS, size=N_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(baseline_value, \n",
    "                shap_values[sampled_indices, :], \n",
    "                data[sampled_indices, :], \n",
    "                feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Outliers are extreme values that deviate from other observations on data. It's useful to understand the influence of various features for outlier predictions to determine if it's a novelty, an experimental error, or a shortcoming in the model. \n",
    "\n",
    "Here we show force plot for prediction outliers that are on either side of the baseline value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top outliers\n",
    "from scipy import stats\n",
    "N_OUTLIERS = 3  # number of outliers on each side of the tail\n",
    "\n",
    "shap_sum = np.sum(shap_values, axis=1)\n",
    "z_scores = stats.zscore(shap_sum)\n",
    "outlier_indices = (np.argpartition(z_scores, -N_OUTLIERS)[-N_OUTLIERS:]).tolist()\n",
    "outlier_indices += (np.argpartition(z_scores, N_OUTLIERS)[:N_OUTLIERS]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fig_index, outlier_index in enumerate(outlier_indices, start=1): \n",
    "    shap.force_plot(baseline_value, \n",
    "                    shap_values[outlier_index, :], \n",
    "                    data[outlier_index, :], \n",
    "                    matplotlib=True, \n",
    "                    feature_names=feature_names) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
